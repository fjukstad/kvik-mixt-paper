In this section we motivate our microservice approach by describing how we
developed a web application for exploring and comparing transcriptional profiles
from blood and tumor samples, called MIxT Blood-Tumor. We describe the process
from initial data analysis to the final application, highlighting the importance
of language-agnostic services to facilitate the use of different tools in
different parts of the application. This is especially important in
interdisciplinary teams where researchers use a wide range of tools. We believe
many system biology data exploration applications are developed similarly and
that they can therefore also benefit from the microservice approach. 

% Bruke MIxT til å fortelle hvordan de forskjellige delene ble gjort og hvordan
% man kan bygge en app uten å gjore alt på nytt igjen, eller hvertfall kunne
% bygge på det man har. 
% 1) les inn data (bioconductor) 
% 2) se på de slå sammen, kanskje enkle vis
% 3) gjøre analyser
% 4) koble resultat opp mot db
% 5) gå gjennom resultatene og db interaktivt.
% 6) del med andre 

\subsection*{Analyzing biological datasets} 
We analyzed profiled RNA in blood and matched tumor from 173 patients in the
Norwegian Women and Cancer (NOWAC) study. Each profile measures the expression
of 16 782 unique genes. 

% BF: Hei LA var dette noe sånn du så for deg? 
We used R and various packages from Bioconductor and CRAN to pre-process and
analyze the gene expression dataset. 
From the initial analyses we built an R package that makes
it possible to share results between researchers within, and outside our group.
To build an application on top of the analyses there are two possibilities: i)
develop a script that generates a static output file (e.g. generate a
html report or CSV files that contain the result data) and build an application
that uses this static data; or ii) use a framework such as Shiny or OpenCPU to
create a dynamic application that interface directly with the R code. To update
an application that builds on top of a static output file
requires manual execution of the output script, while using Shiny or OpenCPU
this task.  

% Ville ikke sagt "we typically". Men heller beskrevet konkret hva som ble gjort
% for MIxT. Generealisering kan gjøres i neste avsnitt der microservice apporach
% blir beskrevet.  Også ville jeg likt å se tall her. Hvor stort er datasettet?
% Hvor lang tid tar det? Er dette i critial path for data exploration eller bare
% pre-processing? 
We typically start off with a messy dataset that needs to go through
several stages of clean-up and preprocessing before we can analyze it.
% Hvordan gjøres dette? Er dette egentlig protptyping, og dermed en slags
% requirements spec for de endelige visualiseringene? Kan dette gjenbrukes eller
% er det mest bruk-og-kast? Hvem gjør dette? 
After the
preprocessing we typically develop some simple visualizations that help discover 
simple patterns in the data. 
% Apply or develop for MIxT?
After this quick dirty data exploration we start to
apply more advanced statistical methods to look for more intricate patterns in
the data.
% Kommer litt overaskende siden det aldri ble sagt hva formålet med dette var :)
% Database lookup for hva?
After this analysis we often end up with genes or lists of genes of
interest that we can use to guide database lookup.
% Hva med non-quirky visualiseringer integrert med database lookup?
% Hva med visualiseringer for andre brukere? Og data?
% Hva med neste verktøy, dvs MIxT for eksempel for methylation + gene
% expression?

% Savner en diskusjon om performance, resource usage, og andre krav til MiXT

% Tror det er enklere å henge med hvis dette blir flettet inn i teksten over
In terms of data analysis code, the preprocessing steps typically consist of
one or more R scripts that we knit \cite{knitr} into PDF reports that we can
revisit later. From these scripts we end up with analysis-ready datasets that
can be shared within the group. The remaining downstream analysis often starts
out in scripts, that are built into R packages with analysis code that can be
shared between researchers. 

\subsection{Kvik micoservices}
% Noe av dette er kanskje allerede i "Building applications"

Based on the development of MiXT and other data exploration tools, we have
generalized our experience into the following design principle guidelines and
microservices provided by the Kvik framework:

Principle 1: language-agnostic (samme har de funnet ut i blant annet Facebook for Thrift)
Principle 2:

Microservice 1: Databases...
Microservice 2: Statistical analyses...

% Også er det viktig å ikke glemme "system aspects" performance, management,
% deployment, etc for disse. Det kn enten forklares her eller senere.

% Det er ikke forklart hvordan ting henger sammen i Kvik, så dette er vanskelig å forstå
With this process in mind, we designed the interface to the R programming
language in Kvik. We want to make it possible to call any function from an R
package and return its results either as plain text, such as comma-separated
tables, or binaries such as images. Enforcing that R code is built into R
packages ensures that the analysis code can be used by power users through an
ordinary R session or in the data exploration application itself. 

\subsection*{Databases} 

Similar to how our analysis process shaped the R interface, it also defined how
we want to build interfaces to online databases. 

% Kan også beskrive hva slags interfaces de eskponerer, hva som er performance
% characteristics, programmerings utforderinger, og tilatt bruk
In its initial state we wanted an interface to interactively query databases
such as KEGG or MSigDB for up-to-date information about genes, gene sets or
biological pathways. This interface should be available within the data
exploration applications to provide valuable metadata, such as gene summaries,
for the researchers exploring results.  

% TODO: Describe the interfaces/API. 
% + abstraksjoner som tar seg av caching og provenance management

% Jeg ser for meg at det er nyttig å kunne si for alle database oppslag noe sånt
% som: read cached value = False, cache result = "session". Dvs alltid les
% nyeste verdi, men cache resultatene for denne session. Kanskje er det også
% andre database-generisk operasjoner som er nyttige abstraksjoner (hent alle
% entries i en liste). Også er det sikkert mulig å pakke disse inn i en
% interface som kan brukes til å implementere database spesifike (KEGG, MsigDB)
% komponenter.


\subsection*{Building applications} 

% Snu om setningene: microservices som lar implemntere i multiple ways
With Kvik there are multiple ways developers can build data
exploration applications. Either bundle analysis and database lookup on a single
computer, or separate computational tasks to more powerful compute clusters to
improve performance. 
In this paper we discuss how to develop applications that follow a
microservices architecture where data analysis and storage is simply a service. 

% Dette er kanskje en av flere microservices?
In Kvik we use R packages as the fundamental building block for data exploration
applications. They provide an interface to data and analyses, and especially in
the field of systems biology, the R programming language provide the largest
collection of data analysis packages. % litt vagt kanskje? 
We discovered that the most sensible way to build applications on top of our
existing code base was to build a system that could interface with our analysis
code directly. In Kvik we built an HTTP interface on top of R that allows users
to call functions and get results using any programming language with an HTTP
library. This allows developers to build data exploration applications in the
programming language that is most suitable, or has the best support, for
presenting that specific data type. 

\subsection*{Statistical analyses}
\emph{Describe how we've designed the interface with R: Build an R-package and
call functions from it, we provide four different output formats to the user
(json, csv, pdf, png),  as well as four different http endpoints (call, get and
rpc).}

% Hva er fordelene med å gjøre det i go?
The R interface in Kvik follows many of the design patterns in OpenCPU. Both
systems interface with R packages using a hybrid state pattern over HTTP. Both
systems provide the same interface to execute analyses and retrieve results.
While OpenCPU is implemented on top of R and Apache, Kvik is implemented from
the ground up in Go. Because of the similarities in the interface to R in Kvik
we provide packages for interfacing with our own R server or OpenCPU R servers
through the \emph{gopencpu} package. 

The R server in Kvik builds on the standard \emph{http} library in Go. On start
it launches a user-defined number of R sessions that execute analyses on demand.
This allows for parallel execution of analyses. We provide a simple FIFO queue
for queuing of requests. The R server also provides the opportunity for users to
cache analysis results to speed up subsequent calls. 

The Kvik R server is suitable for applications where the analysis should be run
on a different server than the web-server hosting the application. If users want
to bundle both the application and R server on the same machine, the \emph{r}
package in Kvik provides this functionality. Although this is possible, we
believe that following a modular approach separating analysis and
application user-interface makes a cleaner and easier to maintain application. 

\subsection*{Databases}
Describe the interface to the databases and what we use it for. Could be
interesting to talk about provenance/caching.

% LAB: her er stedet for alle Go bibliotek og andre lavnivå detlajer
\subsection*{Implementation}

% LAB: Litt usikker på om dette hører til i Results eller Methods
\subsection*{Applications}

% LAB: kort beskrivelse av hva alle apps gjør

% LAB: Figur som viser hva som er felles og ulikt for alle appene. Her bør noe
% være likt ellers har vi bare 3-4 applikasjoner :)

% LAB: mer detaljert beskrivelse av hvordan hver app er implementert med Kvik


